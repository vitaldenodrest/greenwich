\documentclass{beamer}

\usepackage{graphicx}
\usepackage{listings}
\usepackage[backend=biber,style=verbose,sorting=ynt]{biblatex}

\usepackage{derivative}
\usepackage{subfig}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usepackage{amsmath}
\usepackage{stmaryrd}
\usepackage{subfiles} % Best loaded last in the preamble

\addbibresource{references.bib}

\usetheme{cs}


% Document data for the title page
\title{Galerkine methods for the 1D Helmholtz equation}
\subtitle{A deep dive into complex PDEs and FEM with a troublesome example}
\author[V.~de Nodrest]{V.~de Nodrest\inst{1}}
\institute[UFT/VFU]{
  \inst{1} CentraleSupélec, Université Paris Saclay
}
\date{\today}


% Definir o nome do orientador
\newcommand{\advisorname}{\textbf{Orientador:}  Prof. Dr. Nome do Orientador}


% The next block of commands puts the table of contents at the beginning of each section and highlights the current section
\AtBeginSection[]
{
  \begin{frame}
    \frametitle{Table of Contents}
    \tableofcontents[currentsection]
  \end{frame}
}


\begin{document}


\titlegraphic{\logocstext \hspace{2em} \logogretext}

\frame{\titlepage}

% Insert the general toc
\begin{frame}{Table of Contents}
  \tableofcontents    
\end{frame}

\section{Motivation}
\subfile{sections/1-motivation.tex}

\section{Our problem}
\subfile{sections/2-problem.tex}


\begin{frame}[fragile]
  \frametitle{Weak formulation (1)}

  Let's forget about our exact solution and apply conventional PDE tools,
  which are mandatory for harder problems and numerical methods. \\
  \vspace{0.6 cm}
  We will assume that $u$ is a solution and is in $H^2(]0,1[, \mathbb{C})$ (the $\mathbb{C}$ might be omitted in the next slides).\\
  \vspace{0.6 cm}
  Then, for every $v$ in $H^1(0,1)$ we multiply the equation in our domain by $\overline{v}$ and integrate over the domain, yielding:
  $$
  \int_{]0,1[}^{} u'' \overline{v} + k^2 \int_{]0,1[}^{} u \overline{v} = 0
  $$



\end{frame}


\begin{frame}[fragile]
  \frametitle{Weak formulation (2)}

  Integrating by parts, noting:\\
  $\langle u, v \rangle_{L^2} = \int_{]0,1[}^{} u \overline{v}$ (scalar product for the energy norm over our domain)\\
  and using the boundary conditions, we get:
  \begin{block}{Our weak formulation}
    \vspace{-0.6cm}
    \begin{align*}
      \forall v \in H^1(0,1), ~ k^2 \langle u, v \rangle_{L^2} - \langle u', v' \rangle_{L^2} +  ik \left[ u(1) \overline{v(1)} - \overline{v(0)} \right] &= 0
    \end{align*}
    \vspace{-0.6cm}
  \end{block}

  This is only one of the many possible weak formulations we could have obtained.
  Other choices could have been made regarding:
  \begin{itemize}
    \item The space to which $u$ belongs (test functions)
    \item The space to which $v$ belongs (weighting functions)
    \item The norm and the scalar product
  \end{itemize}


\end{frame}


\begin{frame}[fragile]
  \frametitle{Variational formulation (1)}

  We need to rewrite our weak formulation in the standardized form:
  \begin{alertblock}{A variational formulation}
    Find $u \in V_1$ such that
    \vspace{-0.3cm}
      \begin{align*}
        \forall v \in V_2, ~ a(u, v) = l(v)
      \end{align*}
      \vspace{-0.6cm}
  \end{alertblock}  
  Where:
  \begin{itemize}
    \item $V_1$ and $V_2$ are Hilbert spaces (with their respective scalar products)
    \item $a$ is sesquilinear
    \item $l$ is antilinear or linear
  \end{itemize}
  Other properties can work, but this covers most problems.
  Real-valued problems can be seen a trivial particular case.

\end{frame}


\begin{frame}[fragile]
  \frametitle{Variational formulation (2)}

  From the weak formulation, we deduce:
  \begin{block}{Our sesquilinear form}
  \vspace{-0.6cm}
    \begin{align*}
      a : (H^1(0,1))^2 &\longrightarrow \mathbb{C} \\
      (u, v) &\longmapsto k^2 \langle u, v \rangle_{L^2} - \langle u', v' \rangle_{L^2} +  ik \left[ u(1) \overline{v(1)} \right]
    \end{align*}
    \vspace{-0.6cm}
  \end{block}

  \begin{alertblock}{Our antilinear form}
    \vspace{-0.6cm}
      \begin{align*}
        l : H^1(0,1) &\longrightarrow \mathbb{C} \\
        v &\longmapsto ik \left[ \overline{v(0)} \right]
      \end{align*}
      \vspace{-0.6cm}
    \end{alertblock}

  We supposeed $u \in H^2$ to obtain the weak formulation but our forms can be expressed in $H^1$ spaces.
  Still, a solution to the strong problem must be two-times derivable.

\end{frame}


\begin{frame}[fragile]
  \frametitle{Continuity of the sesquilinear form (1)}

  A nice property a sesquilinear form $a : V^2 \rightarrow \mathbb{C}$ can have is continuity:
  \begin{block}{Continuity of a sesquilinear form}
    \vspace{-0.6cm}
      \begin{align*}
        \exists C_a \in \mathbb{R},
        ~
        \forall u,v \in V,
        ~
        \left| a(u,v) \right|
        \le
        C_a
        \lvert \lvert u \rvert \rvert_V
        \lvert \lvert v \rvert \rvert_V
      \end{align*}
      \vspace{-0.6cm}
    \end{block}
  \vspace{0.6 cm}
  Let $u,v \in H^1(0,1)$. Proving that $a$ is continuous starts with the triangle inequality over the complex module:
  $$
  \left| (u, v) \right|
  \le
  \left| k \right|^2
  \left| \langle u, v \rangle_{L^2} \right|
  +
  \left| \langle u', v' \rangle_{L^2} \right|
  +
  \left| k \right|
  \left| u(1) \overline{v(1)} \right|
  $$

\end{frame}


\begin{frame}[fragile]
  \frametitle{Continuity of the sesquilinear form (2)}

  The first step is applying the Cauchy-Schwartz inequality to the $L^2$ products:
  $$
  \left|(u, v)\right|
  \le
  \left|k\right|^2
  \lvert \lvert u \rvert \rvert_{L^2}
  \lvert \lvert v \rvert \rvert_{L^2}
  +
  \lvert \lvert u' \rvert \rvert_{L^2}
  \lvert \lvert v' \rvert \rvert_{L^2}
  +
  \left| k \right| \left| u(1) \overline{v(1)} \right|
  $$

  Using the follwing $k$-dependent scalar product over $H^1(0,1)$ is convenient:
  $$
  \langle u,v \rangle_{1,k}
  =
  \langle u',v' \rangle_{L^2}
  +
  \left| k \right|^2
  \langle u,v \rangle_{L^2}
  ~ \text{thus} ~
  \lvert \lvert u \rvert \rvert_{1,k}^2
  =
  \lvert \lvert u' \rvert \rvert_{L^2}^2 + \left| k \right|^2 \lvert \lvert u \rvert \rvert_{L^2}^2
  $$



  Now, consider two vectors of $\mathbb{R}^2$ whose usual scalar product appears above:
  $$
  \left(
    \left| k \right| \lvert \lvert u \rvert \rvert_{L^2},
    ~
    \lvert \lvert u' \rvert \rvert_{L^2}
  \right)
  ~ \text{and} ~
  \left(
    \left| k \right| \lvert \lvert v \rvert \rvert_{L^2},
    ~
    \lvert \lvert v' \rvert \rvert_{L^2}
  \right)
  $$

  Applying the Cauchy-Schwartz inequality to those $\mathbb{R}^2$ scalar products yields:
  $$
  \left|(u, v)\right|
  \le
  \sqrt{
    \left| k \right|^2 \lvert \lvert u \rvert \rvert_{L^2}^2
    +
    \lvert \lvert u' \rvert \rvert_{L^2}^2
  }
  \sqrt{
    \left| k \right|^2 \lvert \lvert v \rvert \rvert_{L^2}^2
    +
    \lvert \lvert v' \rvert \rvert_{L^2}^2
  }
  +
  \left| k \right| \left| u(1) \overline{v(1)} \right|
  $$


\end{frame}


\begin{frame}[fragile]
  \frametitle{Continuity of the sesquilinear form (3)}

  After all of this, the $k$-dependant norm appears:
  $$
  \left|(u, v)\right|
  \le
  \lvert \lvert u \rvert \rvert_{1,k}^2
  \lvert \lvert v \rvert \rvert_{1,k}^2
  +
  \left| k \right| \left| u(1) \overline{v(1)} \right|
  $$

  \vspace{0.6cm}

  Now, the trace theorem helps us deal with $\left| u(1) \overline{v(1)} \right| = \left| u(1) \right| \left| v(1) \right|$:
  $$
  \left| u(1) \right| \le C_{\gamma} \lvert \lvert u \rvert \rvert_{H^1}
  ~ \text{and} ~
  \left| v(1) \right| \le C_{\gamma} \lvert \lvert v \rvert \rvert_{H^1}
  $$

  \vspace{0.6cm}

  Where the usual $H^1(0,1)$ norm is
  $
  \lvert \lvert u \rvert \rvert_{H^1}^2
  =
  \lvert \lvert u' \rvert \rvert_{L^2}^2
  +
  \lvert \lvert u \rvert \rvert_{L^2}^2
  $
  and the trace constant $C_{\gamma}$ only depends on the domain ($C_{\gamma, 1}$ for our 1D domain of Lebesgue measure $1$).

\end{frame}


\begin{frame}[fragile]
  \frametitle{Continuity of the sesquilinear form (4)}

  Finally, those trace results need to be linked to the $k$-dependant norm.

\end{frame}


\begin{frame}[fragile]
  \frametitle{Continuity of the antilinear form}
  A nice property an antilinear form $l : V \rightarrow \mathbb{C}$ can have is continuity:
  \begin{block}{Continuity of an antilinear form}
    \vspace{-0.6cm}
      \begin{align*}
        \exists C_l \in \mathbb{R}, ~ \forall v \in V, ~ \left| l(v) \right| \le C_l \lvert \lvert v \rvert \rvert_V
      \end{align*}
      \vspace{-0.6cm}
    \end{block}

  Let $v \in H^1(0,1)$. The following expression need to be studied:
  $$
  \left| l(v) \right|
  =
  \left| k \right| \left| v(0) \right|
  $$
  
  Once again, the trace theorem provides an interesting result for the $H^1$ norm:
  $$
  \left| v(0) \right|
  \le
  C_{\gamma} \lvert \lvert v \rvert \rvert_{H^1}
  $$

\end{frame}


\begin{frame}[fragile]
  \frametitle{Coercivity of the sesquilinear form}
\end{frame}


\begin{frame}[fragile]
  \frametitle{Lax-Milgram theory}

  After proving all of those properties and if $a$ was coercive,
  we could have used:
  \begin{alertblock}{The adequate complex-valued version of the Lax-Milgram theorem}
    If:
    \begin{itemize}
      \item $(V, \langle ., . \rangle_V)$ is a Hilbert space with any valid scalar produt
      \item $a: V^2 \rightarrow \mathbb{C}$ is sesquilinear (with antilinearity on the second argument)
      \item $l: V \rightarrow \mathbb{C}$ is antilinear
      \item $a$ and $l$ are continuous (let's name the constants $C_a$ and $C_l$)
      \item $\Re(a)$ is coercive (let's name the constant $\alpha$)
    \end{itemize}
    Then the problem "find $u \in V$ such that for all $v \in V$, $a(u,v)=l(v)$":
    \begin{itemize}
      \item has un unique solution $u$
      \item $\lvert \lvert u \rvert \rvert_{V} \leq \frac{1}{\alpha} \lvert \lvert l \rvert \rvert_{V'} = \frac{C_l}{\alpha}$ 
    \end{itemize}
  \end{alertblock}
  Note that other versions of the theorem exist and that some properties were only chosen as conventions.

\end{frame}


\begin{frame}
  \frametitle{Fredholm theory}

  However, $a$ is not coercive

\end{frame}


\begin{frame}[fragile]
  \frametitle{Well-posedness of our problem}
\end{frame}


\begin{frame}[fragile]
  \frametitle{Galerkin equation}

  Straightforward numerical solving of the variational formulation is not possible,
  as there are infinitely many possible trial functions and weighting functions.\\
  We discretize those spaces, thus yielding:
  \begin{block}{A Galerkin equation}
    Find $u^h \in V_{1}^{h}$ such that
    \vspace{-0.3cm}
      \begin{align*}
        \forall v^h \in V_{2}^{h}, ~ a(u^h, v^h) = l(v^h)
      \end{align*}
      \vspace{-0.6cm}
  \end{block}  
  Where $V_{1}^{h} \subset V_1$ and $V_{2}^{h} \subset V_2$ are finite.\\
  Be reminded that in our case, $V_1 = V_2 = H^1(0,1)$, with the $L^2$ scalar product.\\
  One might notice that the error is orthogonal to the subspaces: 
  $$
  a(u - u^h, v^h) = a(u, v^h) - a(u^h, v^h) = f(v^h) -f(v^h) = 0
  $$
\end{frame}


\begin{frame}[fragile]
  \frametitle{Lax-Milgram Galerkin theory}
\end{frame}


\begin{frame}[fragile]
  \frametitle{Fredholm Galerkin theory}
\end{frame}


\begin{frame}[fragile]
  \frametitle{Matrix form of the Galerkin equation (1)}
  Trivial extension of Galerkine methods to complex-valued problems would be conducted by expressing functions of $V_{1}^{h}$ and $V_{2}^{h}$ as linear combinations of \alert{complex functions} (base of the discrete Hilbert spaces) with \alert{real coefficients}.\\
  \vspace{0.6 cm}
  However, for convenience, we will rather use \alert{complex coefficients} and \alert{real functions}.
  This approach is possible for complex Hilbert spaces that are complexifications of real Hilbert spaces, which is true in our case.\\
  \vspace{0.6 cm}
  The only consequence of such a choice is that we must choose the same dicretization for the real and imaginary parts of our trial and weighting functions.\\
  \vspace{0.6 cm}
  Let's note $(e_{1,i})_{i \in \llbracket 1, n \rrbracket}$ and $(e_{2,i})_{i \in \llbracket 1, m \rrbracket}$ our bases of $\Re V_1^h$ and $\Re V_2^h$.


\end{frame}


\begin{frame}[fragile]
  \frametitle{Matrix form of the Galerkin equation (2)}

  It can easily be shown that to solve the Galerkin problem, it is sufficient to test the solution over every single weighting (test) function.\\
  \vspace{0.6 cm}
  Thus, the Galerkin problem is equivalent to:\\
  Find $(u_i)_i \in \mathbb{C}^{n}$ such that:
  \vspace{-0.3 cm}
  $$
      \forall j \in \llbracket 1, m \rrbracket, ~ a \left(\sum_{i=1}^{n} u_i e_{1,i} ~ , ~ e_{2,j}\right) = l(e_{2,j})
  $$
  Note: we wrote $u^h$ in its base, i.e $u^h = \sum_{i=1}^{n} u_i e_{1,i}$.\\
  \vspace{0.6 cm}
  In our case, $a$ being linear in its first argument, the system can be written as:
  $$
  \forall j \in \llbracket 1, m \rrbracket, ~ \sum_{i=1}^{n} u_i a \left(e_{1,i} ~ , ~ e_{2,j}\right) = l(e_{2,j})
  $$

\end{frame}


\begin{frame}[fragile]
  \frametitle{Matrix form of the Galerkin equation (3)}

  Finally, the system of equations can be written as a matrix product:
  \begin{alertblock}{The matrix form of a Galerkin equation}
    Find $U \in \mathcal{M}_{n,1}(\mathbb{C})$ such that
    \vspace{-0.3cm}
      \begin{align*}
        A^{\mathsf{T}} U = L
        \vspace{-0.3 cm}
      \end{align*}

    Where:
    \begin{itemize}
      \item $\forall (i, j) \in \llbracket 1, n \rrbracket \times \llbracket 1, m \rrbracket, ~ [A]_{i,j} = a \left(e_{1,i} ~ , ~ e_{2,j}\right)$
      \item $\forall j \in \llbracket 1, m \rrbracket, ~ [L]_j = l(e_{2,j})$
    \end{itemize}
  \end{alertblock}

\end{frame}


\begin{frame}[fragile]
  \frametitle{Mesh (1)}

  The first step to finite element methods is the choice of the mesh.
  A mesh is composed of \alert{nodes} that circumscribe \alert{elements}. \\
  \vspace{0.6 cm}
  Some properties are often required for a mesh composed of closed elements $(\Omega_i)_i$ over a domain $\Omega$:
  \begin{align*}
    \bigcup_{i} \Omega_i &= \overline{\Omega} \\
    \forall i \ne j, ~ \Omega_i \cap \Omega_j &= \delta \Omega_i \cap \delta \Omega_j
  \end{align*}
  This means that nothing more but the whole domain is covered by the elements, and that elements only enventually overlap on their boundary (a hyperplane of inferior dimension).


\end{frame}


\begin{frame}[fragile]
  \frametitle{Mesh (2)}

  Mesh design is a whole discipline in itself. However, for the sake of simplicity,
  we will use a uniform mesh of element size $h$ over our unit interval.

  \begin{figure}
    \centering
    \subfloat[Iterations of a 3D mesh\label{fig:2a}]{
      \includegraphics[width=0.4\textwidth]{wrench.jpg}
    }
    \qquad
    \subfloat[1D uniform mesh (an asterisk is a node)\label{fig:2b}]{
      \pgfplotsset{width=0.4\textwidth}
      \begin{tikzpicture}
        \begin{axis}[
          xlabel={$x$}
        ]
          \addplot[
            color=blue,
            domain=0:1,
            samples=10,
            mark=asterisk,
          ]{
            0
          };
        \end{axis}
      \end{tikzpicture}
    }
  \label{fig:2}
  \end{figure}




\end{frame}


\begin{frame}
  \frametitle{Linear interpolation}

  For our 

\end{frame}


\begin{frame}[fragile]
  \frametitle{Filling in the matrices (1)}

  As mentioned before, the matrices $A$ and $L$ need to be computed before the matrix form of the Galerkin equation can be solved.\\
  Depending on the mesh 

\end{frame}




  
\section{Bibliography}
  
\begin{frame}[t,allowframebreaks]
  \frametitle{Bibliography}
  \printbibliography
\end{frame}


\section{Appendix}

\subsection{Appendix A}

\begin{frame}[fragile]
  \frametitle{Appendix A (1)}  

  We will now prove that, in order to prove that any $u^h \in V_{1}^{h}$
  is a solution of the problem for every $v^h \in V_{2}^{h}$,
  it is sufficient to show it is a solution for every base function of $V_{2}^{h}$.\\
  \vspace{0.6 cm}
  Writing $v^h$ in its basis, this can be written as:
  \begin{align*}
    \forall j \in \llbracket 1, m \rrbracket, ~ &a \left(u^h, ~ e_{2,j}\right) = l(e_{2,j}) \\
    &\Leftrightarrow \\
    \forall (v_j)_j \in \mathbb{C}^m, ~ &a \left(u^h, ~ \sum_{j=1}^{m} v_j e_{2,j}\right) = l \left(\sum_{j=1}^{m} v_j e_{2,j}\right)
  \end{align*}
  
\end{frame}

\begin{frame}[fragile]
  \frametitle{Appendix A (2)}  

  \fbox{$\Leftarrow$} Setting $(v_j)_j$ to $(1, 0, ..., 0)$, $(0, 1, 0 , ..., 0)$, ..., $(0, 0, ..., 0, 1)$ seals the deal.\\
  \vspace{0.6 cm}
  \fbox{$\Rightarrow$}
  Let $(v_j)_j \in \mathbb{C}^m$.
  $$
    a \left(u^h, ~ \sum_{j=1}^{m} v_j e_{2,j}\right) = \sum_{j=1}^{m} ~ \overline{v_j} a \left(u^h, ~ e_{2,j}\right) = \sum_{j=1}^{m} ~ \overline{v_j} l(e_{2,j}) = l \left(\sum_{j=1}^{m} v_j e_{2,j}\right)
  $$
  Respectively because $a$ is right-antilinear, the hypothesis, and $l$ is antilinear.
\end{frame}

\end{document}